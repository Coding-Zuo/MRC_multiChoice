# 海华中文阅读理解
```
⛽️  前40进决赛
🏄  全文最终提交: 5月10日 截止
```

## 当前Tricks
1. Paragraph你提到非常长，那么其实改进的特征编码方，
   1) 能否将多个句子分别编码？
   2) 若1) 可以, 那么可以打乱语序, 扩充样本数据增强----> A. 可以在训练时打乱句子，再进行embedding；B。也可以在数据预处理时就打乱好，扩充多一些数据
   3) 若2)能提升效果，进一步可以想着怎么压缩特征表达-----> C.实际上将多个句子分别embedding后，可以直接相加作为新的paragraph特征，已经有文章论证，A句子concatB句子和 A句子和B句子直接相加，效果差别不大。D. 也可以每个句子分别embedding, 然后你可以尝试调整每个句子的权重，按经验说，为首的句子和末尾的句子更重要，那么可以设置一个阈值，来强调句子的特征表达。如果能压缩paragraph的特征表达，按常理说Q，A，P之间的gap能降低。
   4) LongFormer、InFormer都解决这个问题，怎么解决的可以借鉴。

2. 对抗训练
FGM、FGSM、PGD，若有能力都可以尝试
TensorFlow中FGM, PGD,FreeLB等代码在这里面可以借鉴，如果找不到合适的Torch版本，可以看看这里:
https://github.com/geyingli/unif/blob/master/uf/processing.py#L561

3. 损失函数
损失函数主要解决收敛的问题，有时稍微改改能起到不错的作用, 可以作为调研的方向

4. Co-attention
这个不熟悉，但是有同事聊过Co-attention能解决特征语义融合的充分性问题。我觉得很有前景。

5. 外部数据
    一些比赛会使用额外数据, 多的优质数据肯定能增加性能。甚至比改模型更有效。

6. README
   1. 将搜集资料记录在GITHUB中的README中，2. 并且简单进行描述代码如何使用代码。
